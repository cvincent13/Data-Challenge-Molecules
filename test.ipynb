{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset, AddRWStructEncoding\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from Model import Model\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open('graph_config.json') as f:\n",
    "    graph_config = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = config['model_name']\n",
    "model_type = config['model_type']\n",
    "nout = config['nout']\n",
    "nhid = config['nhid']\n",
    "nb_epochs = config['nb_epochs']\n",
    "batch_size_train = config['batch_size_train']\n",
    "batch_size_test = config['batch_size_test']\n",
    "learning_rate = config['learning_rate']\n",
    "load_graph_pretrained = config['load_graph_pretrained']\n",
    "\n",
    "walk_length = graph_config['walk_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type=='text':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    tokenizer = None\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer, graph_transform=AddRWStructEncoding(walk_length))\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer, graph_transform=AddRWStructEncoding(walk_length))\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = torch.nn.CrossEntropyLoss()\n",
    "def contrastive_loss(v1, v2):\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "  return CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels)\n",
    "\n",
    "model = Model(model_name, nout, nhid, graph_config, load_graph_pretrained=load_graph_pretrained, model_type=model_type)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "lr_scheduler = get_scheduler('cosine', optimizer=optimizer, num_warmup_steps=500, num_training_steps=len(train_loader)*nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 82,493,812\n",
      "    Graph encoder: 375,412 parameters\n",
      "    Text encoder: 82,118,400 parameters\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "graph_params = sum(p.numel() for p in model.graph_encoder.parameters())\n",
    "text_params = sum(p.numel() for p in model.text_encoder.parameters())\n",
    "\n",
    "print(f'Total number of parameters: {total_params:,}')\n",
    "print(f'    Graph encoder: {graph_params:,} parameters')\n",
    "print(f'    Text encoder: {text_params:,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_distilroberta-base__gps_3_64_375m_pretrained__base_'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_m_n = graph_config['graph_model_name']\n",
    "g_l = graph_config['graph_layers']\n",
    "g_h_l = graph_config['graph_hidden_channels']\n",
    "pretrained = ''\n",
    "if load_graph_pretrained:\n",
    "    pretrained = 'pretrained'\n",
    "\n",
    "model_save_name = f'{model_type}_{model_name}__{g_m_n}_{g_l}_{g_h_l}_{graph_params//1000}m_{pretrained}__base_'\n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, losses, device, count_iter, printEvery, time1):\n",
    "    loss = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        if model_type == 'text':\n",
    "            input_ids = batch.input_ids\n",
    "            batch.pop('input_ids')\n",
    "            attention_mask = batch.attention_mask\n",
    "            batch.pop('attention_mask')\n",
    "            graph_batch = batch\n",
    "            \n",
    "            x_graph, x_text = model(graph_batch.to(device), \n",
    "                                    input_ids=input_ids.to(device), \n",
    "                                    attention_mask=attention_mask.to(device))\n",
    "        else:\n",
    "            sentences = batch.text\n",
    "            batch.pop('text')\n",
    "            graph_batch = batch\n",
    "            \n",
    "            x_graph, x_text = model(graph_batch.to(device), \n",
    "                                    sentences=sentences.to(device))\n",
    "\n",
    "        current_loss = criterion(x_graph, x_text)   \n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        loss += current_loss.item()\n",
    "        \n",
    "        count_iter += 1\n",
    "        if count_iter % printEvery == 0:\n",
    "            time2 = time.time()\n",
    "            print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                        time2 - time1, loss/printEvery))\n",
    "            losses.append(loss)\n",
    "            loss = 0 \n",
    "\n",
    "    return losses, count_iter\n",
    "\n",
    "\n",
    "def eval(model, val_loader, criterion, device):\n",
    "    model.eval()       \n",
    "    val_loss = 0        \n",
    "    for batch in val_loader:\n",
    "        if model_type == 'text':\n",
    "            input_ids = batch.input_ids\n",
    "            batch.pop('input_ids')\n",
    "            attention_mask = batch.attention_mask\n",
    "            batch.pop('attention_mask')\n",
    "            graph_batch = batch\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                x_graph, x_text = model(graph_batch.to(device), \n",
    "                                        input_ids=input_ids.to(device), \n",
    "                                        attention_mask=attention_mask.to(device))\n",
    "                current_loss = criterion(x_graph, x_text)   \n",
    "                val_loss += current_loss.item()\n",
    "\n",
    "        else:\n",
    "            sentences = batch.text\n",
    "            batch.pop('text')\n",
    "            graph_batch = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x_graph, x_text = model(graph_batch.to(device), \n",
    "                                        sentences = sentences.to(device))\n",
    "                current_loss = criterion(x_graph, x_text)   \n",
    "                val_loss += current_loss.item()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH 1-----\n",
      "Iteration: 50, Time: 45.8356 s, training loss: 6.7651\n",
      "Iteration: 100, Time: 91.0632 s, training loss: 6.5115\n",
      "Iteration: 150, Time: 135.8789 s, training loss: 6.0522\n",
      "Iteration: 200, Time: 180.8622 s, training loss: 5.0110\n",
      "Iteration: 250, Time: 226.0309 s, training loss: 4.2482\n",
      "Iteration: 300, Time: 271.1350 s, training loss: 3.7098\n",
      "Iteration: 350, Time: 316.7400 s, training loss: 3.3752\n",
      "Iteration: 400, Time: 361.6860 s, training loss: 3.0922\n",
      "Iteration: 450, Time: 407.6856 s, training loss: 2.7404\n",
      "Iteration: 500, Time: 453.9784 s, training loss: 2.5600\n",
      "Iteration: 550, Time: 500.0239 s, training loss: 2.4964\n",
      "Iteration: 600, Time: 546.3326 s, training loss: 2.4903\n",
      "Iteration: 650, Time: 592.8481 s, training loss: 2.2617\n",
      "Iteration: 700, Time: 639.2184 s, training loss: 2.1177\n",
      "Iteration: 750, Time: 684.9796 s, training loss: 1.9738\n",
      "Iteration: 800, Time: 729.2038 s, training loss: 1.9128\n",
      "Iteration: 850, Time: 776.1986 s, training loss: 1.8294\n",
      "Iteration: 900, Time: 821.1359 s, training loss: 1.7636\n",
      "Iteration: 950, Time: 867.3723 s, training loss: 1.7373\n",
      "Iteration: 1000, Time: 908.9512 s, training loss: 1.5876\n",
      "Iteration: 1050, Time: 951.1533 s, training loss: 1.6816\n",
      "Iteration: 1100, Time: 993.4623 s, training loss: 1.4814\n",
      "-----EPOCH 1----- done.  Validation loss:  0.5693761041818061\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/text_distilroberta-base__gps_3_64_375m_pretrained__base_0.pt\n",
      "-----EPOCH 2-----\n",
      "Iteration: 1150, Time: 1096.7741 s, training loss: 1.2643\n",
      "Iteration: 1200, Time: 1141.4849 s, training loss: 1.2905\n",
      "Iteration: 1250, Time: 1182.4095 s, training loss: 1.3046\n",
      "Iteration: 1300, Time: 1222.9845 s, training loss: 1.3180\n",
      "Iteration: 1350, Time: 1264.1981 s, training loss: 1.2253\n",
      "Iteration: 1400, Time: 1305.5449 s, training loss: 1.2261\n",
      "Iteration: 1450, Time: 1346.7968 s, training loss: 1.2495\n",
      "Iteration: 1500, Time: 1387.6305 s, training loss: 1.2162\n",
      "Iteration: 1550, Time: 1432.5946 s, training loss: 1.1679\n",
      "Iteration: 1600, Time: 1474.0664 s, training loss: 1.0989\n",
      "Iteration: 1650, Time: 1515.1335 s, training loss: 1.1449\n",
      "Iteration: 1700, Time: 1559.1586 s, training loss: 1.0932\n",
      "Iteration: 1750, Time: 1600.2882 s, training loss: 1.0320\n",
      "Iteration: 1800, Time: 1641.8942 s, training loss: 1.1120\n",
      "Iteration: 1850, Time: 1682.7598 s, training loss: 1.0121\n",
      "Iteration: 1900, Time: 1723.3350 s, training loss: 1.0661\n",
      "Iteration: 1950, Time: 1764.5533 s, training loss: 0.9728\n",
      "Iteration: 2000, Time: 1805.5334 s, training loss: 1.1114\n",
      "Iteration: 2050, Time: 1845.8933 s, training loss: 0.8961\n",
      "Iteration: 2100, Time: 1887.1298 s, training loss: 0.9690\n",
      "Iteration: 2150, Time: 1928.4522 s, training loss: 0.9405\n",
      "Iteration: 2200, Time: 1969.3834 s, training loss: 0.8923\n",
      "-----EPOCH 2----- done.  Validation loss:  0.356594099069353\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/text_distilroberta-base__gps_3_64_375m_pretrained__base_1.pt\n",
      "-----EPOCH 3-----\n",
      "Iteration: 2250, Time: 2067.0124 s, training loss: 0.7691\n",
      "Iteration: 2300, Time: 2109.3067 s, training loss: 0.7509\n",
      "Iteration: 2350, Time: 2149.8840 s, training loss: 0.7690\n",
      "Iteration: 2400, Time: 2191.4766 s, training loss: 0.7455\n",
      "Iteration: 2450, Time: 2233.9484 s, training loss: 0.7100\n",
      "Iteration: 2500, Time: 2276.1337 s, training loss: 0.7119\n",
      "Iteration: 2550, Time: 2318.8334 s, training loss: 0.6902\n",
      "Iteration: 2600, Time: 2360.5652 s, training loss: 0.6826\n",
      "Iteration: 2650, Time: 2401.9860 s, training loss: 0.6960\n",
      "Iteration: 2700, Time: 2443.7980 s, training loss: 0.8045\n",
      "Iteration: 2750, Time: 2485.1298 s, training loss: 0.6754\n",
      "Iteration: 2800, Time: 2530.2572 s, training loss: 0.7786\n",
      "Iteration: 2850, Time: 2572.0132 s, training loss: 0.7098\n",
      "Iteration: 2900, Time: 2612.9537 s, training loss: 0.7172\n",
      "Iteration: 2950, Time: 2654.2868 s, training loss: 0.7028\n",
      "Iteration: 3000, Time: 2695.0619 s, training loss: 0.6860\n",
      "Iteration: 3050, Time: 2736.2069 s, training loss: 0.6753\n",
      "Iteration: 3100, Time: 2777.0557 s, training loss: 0.6699\n",
      "Iteration: 3150, Time: 2817.9663 s, training loss: 0.6156\n",
      "Iteration: 3200, Time: 2859.0746 s, training loss: 0.6771\n",
      "Iteration: 3250, Time: 2899.6540 s, training loss: 0.5922\n",
      "Iteration: 3300, Time: 2941.0178 s, training loss: 0.6240\n",
      "-----EPOCH 3----- done.  Validation loss:  0.2543486482466155\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/text_distilroberta-base__gps_3_64_375m_pretrained__base_2.pt\n",
      "-----EPOCH 4-----\n",
      "Iteration: 3350, Time: 3041.9032 s, training loss: 0.5283\n",
      "Iteration: 3400, Time: 3082.4990 s, training loss: 0.5765\n",
      "Iteration: 3450, Time: 3123.8070 s, training loss: 0.5438\n",
      "Iteration: 3500, Time: 3170.2723 s, training loss: 0.5273\n",
      "Iteration: 3550, Time: 3213.8872 s, training loss: 0.5592\n",
      "Iteration: 3600, Time: 3254.5669 s, training loss: 0.5553\n",
      "Iteration: 3650, Time: 3296.5036 s, training loss: 0.6004\n",
      "Iteration: 3700, Time: 3337.8278 s, training loss: 0.5497\n",
      "Iteration: 3750, Time: 3379.6334 s, training loss: 0.5658\n",
      "Iteration: 3800, Time: 3422.3740 s, training loss: 0.5352\n",
      "Iteration: 3850, Time: 3466.0655 s, training loss: 0.5022\n",
      "Iteration: 3900, Time: 3506.5563 s, training loss: 0.5714\n",
      "Iteration: 3950, Time: 3546.7027 s, training loss: 0.5972\n",
      "Iteration: 4000, Time: 3587.7236 s, training loss: 0.4671\n",
      "Iteration: 4050, Time: 3628.1817 s, training loss: 0.5306\n",
      "Iteration: 4100, Time: 3668.5365 s, training loss: 0.4936\n",
      "Iteration: 4150, Time: 3709.5207 s, training loss: 0.4747\n",
      "Iteration: 4200, Time: 3750.4936 s, training loss: 0.4668\n",
      "Iteration: 4250, Time: 3790.8476 s, training loss: 0.4362\n",
      "Iteration: 4300, Time: 3831.6224 s, training loss: 0.4717\n",
      "Iteration: 4350, Time: 3873.1675 s, training loss: 0.4636\n",
      "Iteration: 4400, Time: 3913.7104 s, training loss: 0.4644\n",
      "-----EPOCH 4----- done.  Validation loss:  0.24661905274801024\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/text_distilroberta-base__gps_3_64_375m_pretrained__base_3.pt\n",
      "-----EPOCH 5-----\n",
      "Iteration: 4450, Time: 4010.5504 s, training loss: 0.3427\n",
      "Iteration: 4500, Time: 4056.8170 s, training loss: 0.3517\n",
      "Iteration: 4550, Time: 4102.7924 s, training loss: 0.4183\n",
      "Iteration: 4600, Time: 4149.2271 s, training loss: 0.3755\n",
      "Iteration: 4650, Time: 4195.4942 s, training loss: 0.3999\n",
      "Iteration: 4700, Time: 4242.4425 s, training loss: 0.4511\n",
      "Iteration: 4750, Time: 4288.4604 s, training loss: 0.4458\n",
      "Iteration: 4800, Time: 4334.4054 s, training loss: 0.4079\n",
      "Iteration: 4850, Time: 4381.1118 s, training loss: 0.4425\n",
      "Iteration: 4900, Time: 4428.2384 s, training loss: 0.4095\n",
      "Iteration: 4950, Time: 4477.4157 s, training loss: 0.3822\n",
      "Iteration: 5000, Time: 4526.5184 s, training loss: 0.4060\n",
      "Iteration: 5050, Time: 4575.3128 s, training loss: 0.3972\n",
      "Iteration: 5100, Time: 4624.5690 s, training loss: 0.4050\n",
      "Iteration: 5150, Time: 4674.2935 s, training loss: 0.4617\n",
      "Iteration: 5200, Time: 4720.5156 s, training loss: 0.3943\n",
      "Iteration: 5250, Time: 4764.7685 s, training loss: 0.4070\n",
      "Iteration: 5300, Time: 4808.6023 s, training loss: 0.3661\n",
      "Iteration: 5350, Time: 4852.1157 s, training loss: 0.3838\n",
      "Iteration: 5400, Time: 4896.2999 s, training loss: 0.4163\n",
      "Iteration: 5450, Time: 4940.6712 s, training loss: 0.3979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_epochs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-----EPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m     losses, count_iter \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintEvery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, val_loader, contrastive_loss, device)\n\u001b[1;32m     16\u001b[0m     best_validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(best_validation_loss, val_loss)\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, losses, device, count_iter, printEvery, time1)\u001b[0m\n\u001b[1;32m     23\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m criterion(x_graph, x_text)   \n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mcurrent_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "losses = []\n",
    "count_iter = 0\n",
    "time1 = time.time()\n",
    "printEvery = 50\n",
    "best_validation_loss = 1000000\n",
    "\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    print('-----EPOCH {}-----'.format(i+1))\n",
    "    losses, count_iter = train_one_epoch(model, train_loader, contrastive_loss, optimizer, losses, device, count_iter, printEvery, time1)\n",
    "\n",
    "    val_loss = eval(model, val_loader, contrastive_loss, device)\n",
    "    \n",
    "    best_validation_loss = min(best_validation_loss, val_loss)\n",
    "\n",
    "    print('-----EPOCH '+str(i+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )\n",
    "    if best_validation_loss==val_loss:\n",
    "        print('validation loss improved saving checkpoint...')\n",
    "        save_path = os.path.join('./checkpoints', model_save_name+str(i)+'.pt')\n",
    "        torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': losses[-1],\n",
    "        }, save_path)\n",
    "        print('checkpoint saved to: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading best model...\n"
     ]
    }
   ],
   "source": [
    "#save_path = os.path.join('./checkpoints', 'model'+str(4)+'.pt')\n",
    "\n",
    "print('loading best model...')\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "graph_model = model.get_graph_encoder()\n",
    "text_model = model.get_text_encoder()\n",
    "\n",
    "test_cids_dataset = GraphDataset(root='./data/', gt=gt, split='test_cids')\n",
    "test_text_dataset = TextDataset(file_path='./data/test_text.txt', tokenizer=tokenizer)\n",
    "\n",
    "idx_to_cid = test_cids_dataset.get_idx_to_cid()\n",
    "\n",
    "test_loader = DataLoader(test_cids_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "graph_embeddings = []\n",
    "for batch in test_loader:\n",
    "    for output in graph_model(batch.to(device)):\n",
    "        graph_embeddings.append(output.tolist())\n",
    "\n",
    "test_text_loader = TorchDataLoader(test_text_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "text_embeddings = []\n",
    "for batch in test_text_loader:\n",
    "    for output in text_model(batch['input_ids'].to(device), \n",
    "                             attention_mask=batch['attention_mask'].to(device)):\n",
    "        text_embeddings.append(output.tolist())\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "\n",
    "solution = pd.DataFrame(similarity)\n",
    "solution['ID'] = solution.index\n",
    "solution = solution[['ID'] + [col for col in solution.columns if col!='ID']]\n",
    "solution.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
