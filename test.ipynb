{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset, AddRWStructEncoding\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from Model import Model\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = torch.nn.CrossEntropyLoss()\n",
    "def contrastive_loss(v1, v2):\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "  return CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels)\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "graph_model_name = 'gin'\n",
    "\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "walk_length = 20\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer, pre_transform=AddRWStructEncoding(walk_length))\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer, pre_transform=AddRWStructEncoding(walk_length))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nb_epochs = 5\n",
    "batch_size_train = 16\n",
    "batch_size_test = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_test, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "model = Model(model_name=model_name, graph_model_name=graph_model_name, num_node_features=300, nout=768, nhid=300, graph_hidden_channels=300, graph_layers=3) # nout = bert model hidden dim\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "epoch = 0\n",
    "loss = 0\n",
    "losses = []\n",
    "count_iter = 0\n",
    "time1 = time.time()\n",
    "printEvery = 50\n",
    "best_validation_loss = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 67,226,148\n",
      "    Graph encoder: 863,268 parameters\n",
      "    Text encoder: 66,362,880 parameters\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "graph_params = sum(p.numel() for p in model.graph_encoder.parameters())\n",
    "text_params = sum(p.numel() for p in model.text_encoder.parameters())\n",
    "\n",
    "print(f'Total number of parameters: {total_params:,}')\n",
    "print(f'    Graph encoder: {graph_params:,} parameters')\n",
    "print(f'    Text encoder: {text_params:,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased__gin_3_300_863m__base_'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_name = f'{model_name}__{graph_model_name}_{3}_{300}_{graph_params//1000}m__base_'\n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH 1-----\n",
      "Iteration: 50, Time: 34.3453 s, training loss: 5.2524\n",
      "Iteration: 100, Time: 60.0932 s, training loss: 4.2078\n",
      "Iteration: 150, Time: 86.1017 s, training loss: 3.7228\n",
      "Iteration: 200, Time: 112.5527 s, training loss: 3.3896\n",
      "Iteration: 250, Time: 138.4410 s, training loss: 3.0198\n",
      "Iteration: 300, Time: 164.3894 s, training loss: 2.8887\n",
      "Iteration: 350, Time: 190.8463 s, training loss: 2.6419\n",
      "Iteration: 400, Time: 217.2572 s, training loss: 2.4117\n",
      "Iteration: 450, Time: 243.2813 s, training loss: 2.2711\n",
      "Iteration: 500, Time: 269.5218 s, training loss: 2.1473\n",
      "Iteration: 550, Time: 295.9180 s, training loss: 2.0163\n",
      "Iteration: 600, Time: 322.2865 s, training loss: 1.8776\n",
      "Iteration: 650, Time: 348.5860 s, training loss: 1.8327\n",
      "Iteration: 700, Time: 375.1021 s, training loss: 1.8261\n",
      "Iteration: 750, Time: 401.5453 s, training loss: 1.7298\n",
      "Iteration: 800, Time: 427.9165 s, training loss: 1.6739\n",
      "Iteration: 850, Time: 454.1497 s, training loss: 1.5123\n",
      "Iteration: 900, Time: 480.2525 s, training loss: 1.5894\n",
      "Iteration: 950, Time: 506.6113 s, training loss: 1.3758\n",
      "Iteration: 1000, Time: 532.7825 s, training loss: 1.5112\n",
      "Iteration: 1050, Time: 559.2636 s, training loss: 1.4628\n",
      "Iteration: 1100, Time: 585.9740 s, training loss: 1.3444\n",
      "Iteration: 1150, Time: 612.6045 s, training loss: 1.2168\n",
      "Iteration: 1200, Time: 639.0545 s, training loss: 1.2662\n",
      "Iteration: 1250, Time: 665.6074 s, training loss: 1.3753\n",
      "Iteration: 1300, Time: 692.0099 s, training loss: 1.2906\n",
      "Iteration: 1350, Time: 718.3759 s, training loss: 1.1611\n",
      "Iteration: 1400, Time: 744.8585 s, training loss: 1.1545\n",
      "Iteration: 1450, Time: 771.4322 s, training loss: 1.0471\n",
      "Iteration: 1500, Time: 798.2754 s, training loss: 1.0263\n",
      "Iteration: 1550, Time: 825.2919 s, training loss: 0.9744\n",
      "Iteration: 1600, Time: 852.1052 s, training loss: 1.0121\n",
      "Iteration: 1650, Time: 879.0873 s, training loss: 0.9960\n",
      "-----EPOCH 1----- done.  Validation loss:  1.0144980722939334\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/distilbert-base-uncased_gin863m_base_0.pt\n",
      "-----EPOCH 2-----\n",
      "Iteration: 1700, Time: 949.4236 s, training loss: 0.8893\n",
      "Iteration: 1750, Time: 976.6395 s, training loss: 0.8265\n",
      "Iteration: 1800, Time: 1003.2404 s, training loss: 0.8477\n",
      "Iteration: 1850, Time: 1030.2127 s, training loss: 0.7049\n",
      "Iteration: 1900, Time: 1057.2819 s, training loss: 0.8439\n",
      "Iteration: 1950, Time: 1084.3770 s, training loss: 0.8200\n",
      "Iteration: 2000, Time: 1111.4713 s, training loss: 0.8079\n",
      "Iteration: 2050, Time: 1138.6354 s, training loss: 0.8692\n",
      "Iteration: 2100, Time: 1165.5175 s, training loss: 0.7853\n",
      "Iteration: 2150, Time: 1192.3384 s, training loss: 0.7741\n",
      "Iteration: 2200, Time: 1219.3647 s, training loss: 0.7920\n",
      "Iteration: 2250, Time: 1246.3379 s, training loss: 0.7392\n",
      "Iteration: 2300, Time: 1273.2557 s, training loss: 0.8056\n",
      "Iteration: 2350, Time: 1300.2043 s, training loss: 0.6868\n",
      "Iteration: 2400, Time: 1326.9082 s, training loss: 0.7994\n",
      "Iteration: 2450, Time: 1353.5555 s, training loss: 0.7222\n",
      "Iteration: 2500, Time: 1380.5940 s, training loss: 0.6909\n",
      "Iteration: 2550, Time: 1407.6149 s, training loss: 0.8070\n",
      "Iteration: 2600, Time: 1434.8446 s, training loss: 0.7027\n",
      "Iteration: 2650, Time: 1461.9910 s, training loss: 0.6101\n",
      "Iteration: 2700, Time: 1488.5979 s, training loss: 0.6723\n",
      "Iteration: 2750, Time: 1515.5569 s, training loss: 0.6788\n",
      "Iteration: 2800, Time: 1542.6433 s, training loss: 0.6601\n",
      "Iteration: 2850, Time: 1569.6925 s, training loss: 0.6668\n",
      "Iteration: 2900, Time: 1596.9515 s, training loss: 0.6940\n",
      "Iteration: 2950, Time: 1623.9169 s, training loss: 0.6124\n",
      "Iteration: 3000, Time: 1650.7321 s, training loss: 0.6110\n",
      "Iteration: 3050, Time: 1677.9651 s, training loss: 0.6068\n",
      "Iteration: 3100, Time: 1704.9215 s, training loss: 0.6258\n",
      "Iteration: 3150, Time: 1732.0675 s, training loss: 0.6518\n",
      "Iteration: 3200, Time: 1759.0156 s, training loss: 0.6193\n",
      "Iteration: 3250, Time: 1785.8527 s, training loss: 0.6071\n",
      "Iteration: 3300, Time: 1812.4175 s, training loss: 0.5299\n",
      "-----EPOCH 2----- done.  Validation loss:  0.6428732422490914\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/distilbert-base-uncased_gin863m_base_1.pt\n",
      "-----EPOCH 3-----\n",
      "Iteration: 3350, Time: 1882.9640 s, training loss: 0.5357\n",
      "Iteration: 3400, Time: 1909.6741 s, training loss: 0.4854\n",
      "Iteration: 3450, Time: 1936.6208 s, training loss: 0.5515\n",
      "Iteration: 3500, Time: 1963.5964 s, training loss: 0.4600\n",
      "Iteration: 3550, Time: 1990.4962 s, training loss: 0.4863\n",
      "Iteration: 3600, Time: 2017.2811 s, training loss: 0.5267\n",
      "Iteration: 3650, Time: 2043.7916 s, training loss: 0.3996\n",
      "Iteration: 3700, Time: 2070.9545 s, training loss: 0.4297\n",
      "Iteration: 3750, Time: 2098.0598 s, training loss: 0.4915\n",
      "Iteration: 3800, Time: 2125.3445 s, training loss: 0.5205\n",
      "Iteration: 3850, Time: 2152.3876 s, training loss: 0.4255\n",
      "Iteration: 3900, Time: 2182.4305 s, training loss: 0.4595\n",
      "Iteration: 3950, Time: 2210.2322 s, training loss: 0.4232\n",
      "Iteration: 4000, Time: 2239.0376 s, training loss: 0.4373\n",
      "Iteration: 4050, Time: 2266.3399 s, training loss: 0.4402\n",
      "Iteration: 4100, Time: 2293.6358 s, training loss: 0.3979\n",
      "Iteration: 4150, Time: 2320.9287 s, training loss: 0.4648\n",
      "Iteration: 4200, Time: 2348.0097 s, training loss: 0.3881\n",
      "Iteration: 4250, Time: 2375.1101 s, training loss: 0.4660\n",
      "Iteration: 4300, Time: 2402.6690 s, training loss: 0.4452\n",
      "Iteration: 4350, Time: 2429.9235 s, training loss: 0.4382\n",
      "Iteration: 4400, Time: 2457.1852 s, training loss: 0.4897\n",
      "Iteration: 4450, Time: 2484.4814 s, training loss: 0.4784\n",
      "Iteration: 4500, Time: 2511.3406 s, training loss: 0.3691\n",
      "Iteration: 4550, Time: 2538.4008 s, training loss: 0.4056\n",
      "Iteration: 4600, Time: 2565.6081 s, training loss: 0.4216\n",
      "Iteration: 4650, Time: 2593.0098 s, training loss: 0.4443\n",
      "Iteration: 4700, Time: 2620.9518 s, training loss: 0.4610\n",
      "Iteration: 4750, Time: 2648.0573 s, training loss: 0.4488\n",
      "Iteration: 4800, Time: 2675.0116 s, training loss: 0.3617\n",
      "Iteration: 4850, Time: 2702.2332 s, training loss: 0.3773\n",
      "Iteration: 4900, Time: 2729.6568 s, training loss: 0.4076\n",
      "Iteration: 4950, Time: 2757.0108 s, training loss: 0.3763\n",
      "-----EPOCH 3----- done.  Validation loss:  0.4984444642275617\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/distilbert-base-uncased_gin863m_base_2.pt\n",
      "-----EPOCH 4-----\n",
      "Iteration: 5000, Time: 2828.0720 s, training loss: 0.3741\n",
      "Iteration: 5050, Time: 2855.3802 s, training loss: 0.3713\n",
      "Iteration: 5100, Time: 2882.7870 s, training loss: 0.3212\n",
      "Iteration: 5150, Time: 2910.0536 s, training loss: 0.4212\n",
      "Iteration: 5200, Time: 2937.0879 s, training loss: 0.3650\n",
      "Iteration: 5250, Time: 2964.2948 s, training loss: 0.3461\n",
      "Iteration: 5300, Time: 2991.2493 s, training loss: 0.3974\n",
      "Iteration: 5350, Time: 3018.5942 s, training loss: 0.3367\n",
      "Iteration: 5400, Time: 3045.9542 s, training loss: 0.3679\n",
      "Iteration: 5450, Time: 3073.2915 s, training loss: 0.3119\n",
      "Iteration: 5500, Time: 3100.0731 s, training loss: 0.3122\n",
      "Iteration: 5550, Time: 3126.8449 s, training loss: 0.4337\n",
      "Iteration: 5600, Time: 3153.8681 s, training loss: 0.3191\n",
      "Iteration: 5650, Time: 3181.1230 s, training loss: 0.3802\n",
      "Iteration: 5700, Time: 3208.3962 s, training loss: 0.3487\n",
      "Iteration: 5750, Time: 3237.1729 s, training loss: 0.2953\n",
      "Iteration: 5800, Time: 3264.9736 s, training loss: 0.3607\n",
      "Iteration: 5850, Time: 3291.7611 s, training loss: 0.3546\n",
      "Iteration: 5900, Time: 3318.8894 s, training loss: 0.3644\n",
      "Iteration: 5950, Time: 3346.2235 s, training loss: 0.2800\n",
      "Iteration: 6000, Time: 3373.5245 s, training loss: 0.2937\n",
      "Iteration: 6050, Time: 3400.6946 s, training loss: 0.2651\n",
      "Iteration: 6100, Time: 3428.1473 s, training loss: 0.3837\n",
      "Iteration: 6150, Time: 3455.9034 s, training loss: 0.3491\n",
      "Iteration: 6200, Time: 3484.2285 s, training loss: 0.2682\n",
      "Iteration: 6250, Time: 3511.6611 s, training loss: 0.2408\n",
      "Iteration: 6300, Time: 3539.1660 s, training loss: 0.3421\n",
      "Iteration: 6350, Time: 3566.2845 s, training loss: 0.3234\n",
      "Iteration: 6400, Time: 3593.2111 s, training loss: 0.3493\n",
      "Iteration: 6450, Time: 3620.3573 s, training loss: 0.3411\n",
      "Iteration: 6500, Time: 3647.8217 s, training loss: 0.3019\n",
      "Iteration: 6550, Time: 3675.5302 s, training loss: 0.3753\n",
      "Iteration: 6600, Time: 3702.6408 s, training loss: 0.2847\n",
      "-----EPOCH 4----- done.  Validation loss:  0.42450951279370464\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/distilbert-base-uncased_gin863m_base_3.pt\n",
      "-----EPOCH 5-----\n",
      "Iteration: 6650, Time: 3776.2569 s, training loss: 0.3140\n",
      "Iteration: 6700, Time: 3804.5290 s, training loss: 0.2433\n",
      "Iteration: 6750, Time: 3831.0172 s, training loss: 0.2874\n",
      "Iteration: 6800, Time: 3858.2372 s, training loss: 0.2667\n",
      "Iteration: 6850, Time: 3887.4143 s, training loss: 0.3119\n",
      "Iteration: 6900, Time: 3916.0680 s, training loss: 0.2231\n",
      "Iteration: 6950, Time: 3945.3230 s, training loss: 0.1980\n",
      "Iteration: 7000, Time: 3975.5462 s, training loss: 0.2583\n",
      "Iteration: 7050, Time: 4003.7092 s, training loss: 0.2610\n",
      "Iteration: 7100, Time: 4031.7424 s, training loss: 0.2909\n",
      "Iteration: 7150, Time: 4061.6693 s, training loss: 0.2846\n",
      "Iteration: 7200, Time: 4091.0492 s, training loss: 0.2661\n",
      "Iteration: 7250, Time: 4119.3896 s, training loss: 0.2710\n",
      "Iteration: 7300, Time: 4150.2785 s, training loss: 0.2955\n",
      "Iteration: 7350, Time: 4179.6437 s, training loss: 0.2519\n",
      "Iteration: 7400, Time: 4207.9432 s, training loss: 0.2976\n",
      "Iteration: 7450, Time: 4236.2454 s, training loss: 0.3048\n",
      "Iteration: 7500, Time: 4264.4717 s, training loss: 0.3060\n",
      "Iteration: 7550, Time: 4294.0363 s, training loss: 0.2798\n",
      "Iteration: 7600, Time: 4323.7939 s, training loss: 0.2781\n",
      "Iteration: 7650, Time: 4352.6539 s, training loss: 0.2927\n",
      "Iteration: 7700, Time: 4381.4779 s, training loss: 0.2818\n",
      "Iteration: 7750, Time: 4410.3364 s, training loss: 0.2907\n",
      "Iteration: 7800, Time: 4439.3490 s, training loss: 0.2876\n",
      "Iteration: 7850, Time: 4468.2338 s, training loss: 0.2724\n",
      "Iteration: 7900, Time: 4495.5780 s, training loss: 0.2970\n",
      "Iteration: 7950, Time: 4522.6211 s, training loss: 0.2476\n",
      "Iteration: 8000, Time: 4549.7795 s, training loss: 0.2621\n",
      "Iteration: 8050, Time: 4577.1742 s, training loss: 0.3748\n",
      "Iteration: 8100, Time: 4604.0117 s, training loss: 0.2608\n",
      "Iteration: 8150, Time: 4631.6002 s, training loss: 0.2220\n",
      "Iteration: 8200, Time: 4659.0785 s, training loss: 0.2370\n",
      "Iteration: 8250, Time: 4684.9760 s, training loss: 0.2458\n",
      "-----EPOCH 5----- done.  Validation loss:  0.3866772618536176\n",
      "validation loss improved saving checkpoint...\n",
      "checkpoint saved to: ./checkpoints/distilbert-base-uncased_gin863m_base_4.pt\n"
     ]
    }
   ],
   "source": [
    "for i in range(nb_epochs):\n",
    "    print('-----EPOCH {}-----'.format(i+1))\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        \n",
    "        x_graph, x_text = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        current_loss = contrastive_loss(x_graph, x_text)   \n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += current_loss.item()\n",
    "        \n",
    "        count_iter += 1\n",
    "        if count_iter % printEvery == 0:\n",
    "            time2 = time.time()\n",
    "            print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                        time2 - time1, loss/printEvery))\n",
    "            losses.append(loss)\n",
    "            loss = 0 \n",
    "\n",
    "    model.eval()       \n",
    "    val_loss = 0        \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        x_graph, x_text = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        current_loss = contrastive_loss(x_graph, x_text)   \n",
    "        val_loss += current_loss.item()\n",
    "    best_validation_loss = min(best_validation_loss, val_loss)\n",
    "\n",
    "    print('-----EPOCH '+str(i+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )\n",
    "    if best_validation_loss==val_loss:\n",
    "        print('validation loss improved saving checkpoint...')\n",
    "        save_path = os.path.join('./checkpoints', model_save_name+str(i)+'.pt')\n",
    "        torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': loss,\n",
    "        }, save_path)\n",
    "        print('checkpoint saved to: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading best model...\n"
     ]
    }
   ],
   "source": [
    "#save_path = os.path.join('./checkpoints', 'model'+str(4)+'.pt')\n",
    "\n",
    "print('loading best model...')\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "graph_model = model.get_graph_encoder()\n",
    "text_model = model.get_text_encoder()\n",
    "\n",
    "test_cids_dataset = GraphDataset(root='./data/', gt=gt, split='test_cids')\n",
    "test_text_dataset = TextDataset(file_path='./data/test_text.txt', tokenizer=tokenizer)\n",
    "\n",
    "idx_to_cid = test_cids_dataset.get_idx_to_cid()\n",
    "\n",
    "test_loader = DataLoader(test_cids_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "graph_embeddings = []\n",
    "for batch in test_loader:\n",
    "    for output in graph_model(batch.to(device)):\n",
    "        graph_embeddings.append(output.tolist())\n",
    "\n",
    "test_text_loader = TorchDataLoader(test_text_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "text_embeddings = []\n",
    "for batch in test_text_loader:\n",
    "    for output in text_model(batch['input_ids'].to(device), \n",
    "                             attention_mask=batch['attention_mask'].to(device)):\n",
    "        text_embeddings.append(output.tolist())\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "\n",
    "solution = pd.DataFrame(similarity)\n",
    "solution['ID'] = solution.index\n",
    "solution = solution[['ID'] + [col for col in solution.columns if col!='ID']]\n",
    "solution.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH 1----- done.  Validation loss:  0.9059125973262648\n",
      "-----EPOCH 2----- done.  Validation loss:  0.5076332101664112\n",
      "-----EPOCH 3----- done.  Validation loss:  0.3639905517425946\n",
      "-----EPOCH 4----- done.  Validation loss:  0.32216463984858584\n",
      "-----EPOCH 5----- done.  Validation loss:  0.29098984794151306\n"
     ]
    }
   ],
   "source": [
    "for i in range(nb_epochs):\n",
    "    save_path = os.path.join('./checkpoints', 'model'+str(i)+'.pt')\n",
    "    checkpoint = torch.load(save_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()       \n",
    "    val_loss = 0        \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        x_graph, x_text = model(graph_batch.to(device), \n",
    "                                input_ids.to(device), \n",
    "                                attention_mask.to(device))\n",
    "        current_loss = contrastive_loss(x_graph, x_text)   \n",
    "        val_loss += current_loss.item()\n",
    "\n",
    "    print('-----EPOCH '+str(i+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
