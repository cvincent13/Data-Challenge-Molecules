{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from Model import GraphCL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from dataloader import GraphDatasetPretrain, AddRWStructEncoding\n",
    "from dataloader import drop_node_augment, edge_pert_augment, attr_mask_augment, subgraph_augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "walk_length = 20\n",
    "val_dataset = GraphDatasetPretrain(root='./data/', gt=gt, split='val', \n",
    "                                   graph_augment1=attr_mask_augment, graph_augment2=subgraph_augment, \n",
    "                                   aug_p=0.2, graph_transform=AddRWStructEncoding(walk_length))\n",
    "train_dataset = GraphDatasetPretrain(root='./data/', gt=gt, split='train',\n",
    "                                     graph_augment1=attr_mask_augment, graph_augment2=subgraph_augment,\n",
    "                                     aug_p=0.2, graph_transform=AddRWStructEncoding(walk_length))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nb_epochs = 5\n",
    "batch_size_train = 32\n",
    "batch_size_test = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_test, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_config = {}\n",
    "graph_config['graph_model_name'] = 'gps'\n",
    "graph_config['num_node_features'] = 300\n",
    "graph_config['graph_hidden_channels'] = 64\n",
    "graph_config['graph_layer'] = 3\n",
    "graph_config['n_head'] = 4\n",
    "graph_config['n_feedforward'] = 128\n",
    "graph_config['input_dropout'] = 0.1\n",
    "graph_config['dropout'] = 0.0\n",
    "graph_config['attention_dropout'] = 0.25\n",
    "graph_config['conv_type'] = 'Gated'\n",
    "graph_config['walk_length'] = 20\n",
    "graph_config['dim_se'] = 28\n",
    "\n",
    "model = GraphCL(graph_config, 64, 128)\n",
    "model.to(device)\n",
    "\n",
    "CE = torch.nn.CrossEntropyLoss()\n",
    "def contrastive_loss(v1, v2):\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "  return CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=0.01)\n",
    "\n",
    "epoch = 0\n",
    "loss = 0\n",
    "losses = []\n",
    "count_iter = 0\n",
    "time1 = time.time()\n",
    "printEvery = 50\n",
    "best_validation_loss = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH 1-----\n",
      "Iteration: 50, Time: 26.7592 s, training loss: 2.7703\n",
      "Iteration: 100, Time: 53.4157 s, training loss: 0.9629\n",
      "Iteration: 150, Time: 79.4561 s, training loss: 0.6625\n",
      "Iteration: 200, Time: 108.7342 s, training loss: 0.5122\n",
      "Iteration: 250, Time: 135.5258 s, training loss: 0.4553\n",
      "Iteration: 300, Time: 162.2332 s, training loss: 0.3549\n",
      "Iteration: 350, Time: 189.7228 s, training loss: 0.3015\n",
      "Iteration: 400, Time: 215.9638 s, training loss: 0.3520\n",
      "Iteration: 450, Time: 246.3472 s, training loss: 0.2635\n",
      "Iteration: 500, Time: 275.3385 s, training loss: 0.2887\n",
      "Iteration: 550, Time: 303.9189 s, training loss: 0.2533\n",
      "Iteration: 600, Time: 333.6415 s, training loss: 0.1809\n",
      "Iteration: 650, Time: 363.1292 s, training loss: 0.2398\n",
      "Iteration: 700, Time: 390.6797 s, training loss: 0.2583\n",
      "Iteration: 750, Time: 419.9492 s, training loss: 0.2012\n",
      "Iteration: 800, Time: 448.8914 s, training loss: 0.2089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH 1----- done.  Validation loss:  0.1722753430949524\n",
      "validation loss improved saving checkpoint...\n",
      "-----EPOCH 2-----\n",
      "Iteration: 850, Time: 509.9113 s, training loss: 0.2164\n",
      "Iteration: 900, Time: 537.5832 s, training loss: 0.1786\n",
      "Iteration: 950, Time: 566.2843 s, training loss: 0.1811\n",
      "Iteration: 1000, Time: 593.9891 s, training loss: 0.2036\n",
      "Iteration: 1050, Time: 622.9134 s, training loss: 0.1738\n",
      "Iteration: 1100, Time: 652.1620 s, training loss: 0.1581\n",
      "Iteration: 1150, Time: 680.9639 s, training loss: 0.1978\n",
      "Iteration: 1200, Time: 708.9790 s, training loss: 0.1712\n",
      "Iteration: 1250, Time: 738.2047 s, training loss: 0.1596\n",
      "Iteration: 1300, Time: 767.9549 s, training loss: 0.1694\n",
      "Iteration: 1350, Time: 794.8194 s, training loss: 0.1891\n",
      "Iteration: 1400, Time: 822.4557 s, training loss: 0.1443\n",
      "Iteration: 1450, Time: 851.0774 s, training loss: 0.1437\n",
      "Iteration: 1500, Time: 878.8630 s, training loss: 0.1892\n",
      "Iteration: 1550, Time: 908.5150 s, training loss: 0.2465\n",
      "Iteration: 1600, Time: 935.5080 s, training loss: 0.1746\n",
      "Iteration: 1650, Time: 963.4369 s, training loss: 0.1441\n",
      "-----EPOCH 2----- done.  Validation loss:  0.11448569175711153\n",
      "validation loss improved saving checkpoint...\n",
      "-----EPOCH 3-----\n",
      "Iteration: 1700, Time: 1024.4771 s, training loss: 0.1391\n",
      "Iteration: 1750, Time: 1054.4622 s, training loss: 0.1575\n",
      "Iteration: 1800, Time: 1085.4503 s, training loss: 0.1578\n",
      "Iteration: 1850, Time: 1115.1594 s, training loss: 0.1379\n",
      "Iteration: 1900, Time: 1143.5374 s, training loss: 0.1272\n",
      "Iteration: 1950, Time: 1171.7771 s, training loss: 0.1544\n",
      "Iteration: 2000, Time: 1201.4497 s, training loss: 0.1370\n",
      "Iteration: 2050, Time: 1229.3126 s, training loss: 0.1035\n",
      "Iteration: 2100, Time: 1259.0969 s, training loss: 0.1067\n",
      "Iteration: 2150, Time: 1287.9720 s, training loss: 0.1385\n",
      "Iteration: 2200, Time: 1314.0300 s, training loss: 0.1393\n",
      "Iteration: 2250, Time: 1342.4687 s, training loss: 0.1573\n",
      "Iteration: 2300, Time: 1373.6808 s, training loss: 0.1147\n",
      "Iteration: 2350, Time: 1401.7066 s, training loss: 0.1511\n",
      "Iteration: 2400, Time: 1428.7861 s, training loss: 0.2277\n",
      "Iteration: 2450, Time: 1459.1902 s, training loss: 0.1844\n",
      "-----EPOCH 3----- done.  Validation loss:  0.0926534626753262\n",
      "validation loss improved saving checkpoint...\n",
      "-----EPOCH 4-----\n",
      "Iteration: 2500, Time: 1523.7293 s, training loss: 0.1370\n",
      "Iteration: 2550, Time: 1553.3304 s, training loss: 0.1388\n",
      "Iteration: 2600, Time: 1580.1876 s, training loss: 0.1320\n",
      "Iteration: 2650, Time: 1606.4881 s, training loss: 0.1110\n",
      "Iteration: 2700, Time: 1636.2743 s, training loss: 0.1525\n",
      "Iteration: 2750, Time: 1664.6011 s, training loss: 0.1258\n",
      "Iteration: 2800, Time: 1693.5515 s, training loss: 0.1417\n",
      "Iteration: 2850, Time: 1723.0246 s, training loss: 0.1648\n",
      "Iteration: 2900, Time: 1751.5536 s, training loss: 0.1340\n",
      "Iteration: 2950, Time: 1777.5257 s, training loss: 0.1020\n",
      "Iteration: 3000, Time: 1803.7981 s, training loss: 0.1350\n",
      "Iteration: 3050, Time: 1830.6536 s, training loss: 0.1130\n",
      "Iteration: 3100, Time: 1856.1864 s, training loss: 0.1064\n",
      "Iteration: 3150, Time: 1882.9542 s, training loss: 0.1659\n",
      "Iteration: 3200, Time: 1909.6581 s, training loss: 0.1549\n",
      "Iteration: 3250, Time: 1934.6354 s, training loss: 0.1106\n",
      "Iteration: 3300, Time: 1960.6998 s, training loss: 0.0906\n",
      "-----EPOCH 4----- done.  Validation loss:  0.08883783410636426\n",
      "validation loss improved saving checkpoint...\n",
      "-----EPOCH 5-----\n",
      "Iteration: 3350, Time: 2013.6420 s, training loss: 0.1217\n",
      "Iteration: 3400, Time: 2039.1442 s, training loss: 0.1461\n",
      "Iteration: 3450, Time: 2066.0092 s, training loss: 0.1090\n",
      "Iteration: 3500, Time: 2094.5059 s, training loss: 0.1317\n",
      "Iteration: 3550, Time: 2121.0224 s, training loss: 0.1326\n",
      "Iteration: 3600, Time: 2147.9771 s, training loss: 0.1372\n",
      "Iteration: 3650, Time: 2174.6817 s, training loss: 0.1414\n",
      "Iteration: 3700, Time: 2200.9727 s, training loss: 0.1444\n",
      "Iteration: 3750, Time: 2226.9080 s, training loss: 0.1140\n",
      "Iteration: 3800, Time: 2253.3561 s, training loss: 0.1026\n",
      "Iteration: 3850, Time: 2279.4687 s, training loss: 0.1176\n",
      "Iteration: 3900, Time: 2306.3838 s, training loss: 0.0874\n",
      "Iteration: 3950, Time: 2334.9533 s, training loss: 0.0713\n",
      "Iteration: 4000, Time: 2363.9815 s, training loss: 0.0967\n",
      "Iteration: 4050, Time: 2394.1252 s, training loss: 0.1259\n",
      "Iteration: 4100, Time: 2423.0084 s, training loss: 0.1122\n",
      "-----EPOCH 5----- done.  Validation loss:  0.06129451816349711\n",
      "validation loss improved saving checkpoint...\n"
     ]
    }
   ],
   "source": [
    "for i in range(nb_epochs):\n",
    "    print('-----EPOCH {}-----'.format(i+1))\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        aug_batch1, aug_batch2 = batch\n",
    "        aug_batch1 = aug_batch1.to(device)\n",
    "        aug_batch2 = aug_batch2.to(device)\n",
    "\n",
    "        x_1 = model(aug_batch1)\n",
    "        x_2 = model(aug_batch2)\n",
    "        \n",
    "        current_loss = contrastive_loss(x_1, x_2)   \n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += current_loss.item()\n",
    "        \n",
    "        count_iter += 1\n",
    "        if count_iter % printEvery == 0:\n",
    "            time2 = time.time()\n",
    "            print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                        time2 - time1, loss/printEvery))\n",
    "            losses.append(loss)\n",
    "            loss = 0 \n",
    "\n",
    "    model.eval()       \n",
    "    val_loss = 0        \n",
    "    for batch in val_loader:\n",
    "        aug_batch1, aug_batch2 = batch\n",
    "        aug_batch1 = aug_batch1.to(device)\n",
    "        aug_batch2 = aug_batch2.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_1 = model(aug_batch1)\n",
    "            x_2 = model(aug_batch2)\n",
    "\n",
    "            current_loss = contrastive_loss(x_1, x_2)   \n",
    "            val_loss += current_loss.item()\n",
    "\n",
    "    best_validation_loss = min(best_validation_loss, val_loss)\n",
    "\n",
    "    print('-----EPOCH '+str(i+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )\n",
    "    if best_validation_loss==val_loss:\n",
    "        print('validation loss improved saving checkpoint...')\n",
    "        \"\"\"save_path = os.path.join('./checkpoints', model_save_name+str(i)+'.pt')\n",
    "        torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': loss,\n",
    "        }, save_path)\n",
    "        print('checkpoint saved to: {}'.format(save_path))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
