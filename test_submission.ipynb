{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset, AddRWStructEncoding\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from Model import Model\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import label_ranking_average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open('graph_config.json') as f:\n",
    "    graph_config = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = config['model_name']\n",
    "model_type = config['model_type']\n",
    "nout = config['nout']\n",
    "nhid = config['nhid']\n",
    "nb_epochs = config['nb_epochs']\n",
    "batch_size_train = config['batch_size_train']\n",
    "batch_size_test = config['batch_size_test']\n",
    "learning_rate = config['learning_rate']\n",
    "load_graph_pretrained = config['load_graph_pretrained']\n",
    "\n",
    "walk_length = graph_config['walk_length']\n",
    "\n",
    "if model_type=='text':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    tokenizer = None\n",
    "if model_type=='w2v':\n",
    "    model_w2v = gensim.models.KeyedVectors.load_word2vec_format(model_name + '.txt')\n",
    "    w2v_embeddings = np.zeros((len(model_w2v.vectors)+1, model_w2v.vectors.shape[1]), dtype=np.float32)\n",
    "    w2v_embeddings[1:] = model_w2v.vectors\n",
    "    nltk_tokenizer = word_tokenize\n",
    "    word2idx = model_w2v.key_to_index\n",
    "else:\n",
    "    nltk_tokenizer = None\n",
    "    word2idx = None\n",
    "    w2v_embeddings = None\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(model_name, nout, nhid, graph_config, load_graph_pretrained=load_graph_pretrained, \n",
    "              model_type=model_type, w2v_embeddings=w2v_embeddings).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "graph_params = sum(p.numel() for p in model.graph_encoder.parameters())\n",
    "text_params = sum(p.numel() for p in model.text_encoder.parameters())\n",
    "\n",
    "g_m_n = graph_config['graph_model_name']\n",
    "g_l = graph_config['graph_layers']\n",
    "g_h_l = graph_config['graph_hidden_channels']\n",
    "pretrained = ''\n",
    "if len(load_graph_pretrained)>0:\n",
    "    pretrained = 'pretrained'\n",
    "\n",
    "s_name = model_name.replace('/', '-')\n",
    "model_save_name = f'{model_type}_{s_name}__{g_m_n}_{g_l}_{g_h_l}_{graph_params//1000}m_{pretrained}__base2_'\n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer, \n",
    "                               nltk_tokenizer=nltk_tokenizer, word2idx=word2idx, \n",
    "                               graph_transform=AddRWStructEncoding(walk_length))\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading best model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3653687012137659"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = os.path.join('./checkpoints', 'ep'+str(9)+model_save_name+'.pt')\n",
    "\n",
    "print('loading best model...')\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "graph_embeddings = []\n",
    "text_embeddings = []\n",
    "\n",
    "for batch in val_loader:\n",
    "    input_ids = batch.input_ids\n",
    "    batch.pop('input_ids')\n",
    "    attention_mask = batch.attention_mask\n",
    "    batch.pop('attention_mask')\n",
    "    graph_batch = batch\n",
    "    with torch.no_grad():\n",
    "        x_graph, x_text = model(graph_batch.to(device), \n",
    "                                input_ids=input_ids.to(device), \n",
    "                                attention_mask=attention_mask.to(device))\n",
    "        \n",
    "        for output in x_graph:\n",
    "            graph_embeddings.append(output.tolist())\n",
    "        for output in x_text:\n",
    "            text_embeddings.append(output.tolist())\n",
    "\n",
    "similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "y_true = np.identity(len(val_dataset))\n",
    "label_ranking_average_precision_score(y_true, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "/media/cedric/Stockage1/Documents/Cours/MVA/Semestre1/ALTEGRAD/Challenge/Data-Challenge-Molecules/dataloader.py:197: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n",
      "/media/cedric/Stockage1/Documents/Cours/MVA/Semestre1/ALTEGRAD/Challenge/Data-Challenge-Molecules/dataloader.py:197: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join('./checkpoints', 'ep'+str(9)+model_save_name+'.pt')\n",
    "\n",
    "print('loading best model...')\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "graph_model = model.get_graph_encoder()\n",
    "text_model = model.get_text_encoder()\n",
    "\n",
    "test_cids_dataset = GraphDataset(root='./data/', gt=gt, split='test_cids', graph_transform=AddRWStructEncoding(walk_length))\n",
    "test_text_dataset = TextDataset(file_path='./data/test_text.txt', tokenizer=tokenizer, nltk_tokenizer=nltk_tokenizer, word2idx=word2idx)\n",
    "\n",
    "idx_to_cid = test_cids_dataset.get_idx_to_cid()\n",
    "\n",
    "test_loader = DataLoader(test_cids_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "graph_embeddings = []\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():\n",
    "        for output in graph_model(batch.to(device)):\n",
    "            graph_embeddings.append(output.tolist())\n",
    "\n",
    "test_text_loader = TorchDataLoader(test_text_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "text_embeddings = []\n",
    "for batch in test_text_loader:\n",
    "    with torch.no_grad():\n",
    "        for output in text_model(batch['input_ids'].to(device), \n",
    "                                attention_mask=batch['attention_mask'].to(device),\n",
    "                                sentences=None):\n",
    "            text_embeddings.append(output.tolist())\n",
    "\n",
    "\n",
    "similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "\n",
    "solution = pd.DataFrame(similarity)\n",
    "solution['ID'] = solution.index\n",
    "solution = solution[['ID'] + [col for col in solution.columns if col!='ID']]\n",
    "solution.to_csv('submissions/' + model_save_name + '_submissiontest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
