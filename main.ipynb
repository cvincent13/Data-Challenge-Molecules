{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloader import GraphTextDataset, GraphDataset, TextDataset, AddRWStructEncoding\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from Model import Model\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import torch\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open('graph_config.json') as f:\n",
    "    graph_config = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = config['model_name']\n",
    "model_type = config['model_type']\n",
    "pooling_type = config['pooling_type']\n",
    "nout = config['nout']\n",
    "nhid = config['nhid']\n",
    "nb_epochs = config['nb_epochs']\n",
    "batch_size_train = config['batch_size_train']\n",
    "batch_size_test = config['batch_size_test']\n",
    "learning_rate = config['learning_rate']\n",
    "load_graph_pretrained = config['load_graph_pretrained']\n",
    "\n",
    "walk_length = graph_config['walk_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type=='text':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "    tokenizer = None\n",
    "if model_type=='w2v':\n",
    "    model_w2v = gensim.models.KeyedVectors.load_word2vec_format(model_name + '.txt')\n",
    "    w2v_embeddings = np.zeros((len(model_w2v.vectors)+1, model_w2v.vectors.shape[1]), dtype=np.float32)\n",
    "    w2v_embeddings[1:] = model_w2v.vectors\n",
    "    nltk_tokenizer = word_tokenize\n",
    "    word2idx = model_w2v.key_to_index\n",
    "else:\n",
    "    nltk_tokenizer = None\n",
    "    word2idx = None\n",
    "    w2v_embeddings = None\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "\n",
    "val_dataset = GraphTextDataset(root='./data/', gt=gt, split='val', tokenizer=tokenizer, \n",
    "                               nltk_tokenizer=nltk_tokenizer, word2idx=word2idx, \n",
    "                               graph_transform=AddRWStructEncoding(walk_length))\n",
    "train_dataset = GraphTextDataset(root='./data/', gt=gt, split='train', tokenizer=tokenizer, \n",
    "                                 nltk_tokenizer=nltk_tokenizer, word2idx=word2idx, \n",
    "                                 graph_transform=AddRWStructEncoding(walk_length))\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = torch.nn.CrossEntropyLoss()\n",
    "def contrastive_loss(v1, v2):\n",
    "  batch_size = v1.shape[0]\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "  return ((CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels))/2)*(16/batch_size)\n",
    "\n",
    "model = Model(model_name, nout, nhid, graph_config, load_graph_pretrained=load_graph_pretrained, \n",
    "              model_type=model_type, pooling_type=pooling_type, w2v_embeddings=w2v_embeddings)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate,\n",
    "                                betas=(0.9, 0.999),\n",
    "                                weight_decay=config['weight_decay'], \n",
    "                                eps=1e-08)\n",
    "\n",
    "lr_scheduler = get_scheduler('cosine', optimizer=optimizer, num_warmup_steps=config['num_warmup_steps'], \n",
    "                             num_training_steps=len(train_loader)*nb_epochs*config['scheduler_steps_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 82,882,932\n",
      "    Graph encoder: 764,532 parameters\n",
      "    Text encoder: 82,118,400 parameters\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "graph_params = sum(p.numel() for p in model.graph_encoder.parameters())\n",
    "text_params = sum(p.numel() for p in model.text_encoder.parameters())\n",
    "\n",
    "print(f'Total number of parameters: {total_params:,}')\n",
    "print(f'    Graph encoder: {graph_params:,} parameters')\n",
    "print(f'    Text encoder: {text_params:,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_m_n = graph_config['graph_model_name']\n",
    "g_l = graph_config['graph_layers']\n",
    "g_h_l = graph_config['graph_hidden_channels']\n",
    "pretrained = ''\n",
    "if len(load_graph_pretrained)>0:\n",
    "    pretrained = 'pretrained'\n",
    "\n",
    "s_name = model_name.replace('/', '-')\n",
    "model_save_name = f'{model_type}_{s_name}__{g_m_n}_{g_l}_{g_h_l}_{graph_params//1000}m_{pretrained}__base2_'\n",
    "model_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, losses, device, count_iter, printEvery, time1):\n",
    "    loss = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "            \n",
    "        x_graph, x_text = model(graph_batch.to(device), \n",
    "                                input_ids=input_ids.to(device), \n",
    "                                attention_mask=attention_mask.to(device))\n",
    "            \n",
    "\n",
    "        current_loss = criterion(x_graph, x_text)   \n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        loss += current_loss.item()\n",
    "        \n",
    "        count_iter += 1\n",
    "        if count_iter % printEvery == 0:\n",
    "            time2 = time.time()\n",
    "            print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                        time2 - time1, loss/printEvery))\n",
    "            losses.append(loss)\n",
    "            loss = 0 \n",
    "\n",
    "    return losses, count_iter\n",
    "\n",
    "\n",
    "def eval(model, val_loader, criterion, device):\n",
    "    model.eval()       \n",
    "    val_loss = 0        \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch.input_ids\n",
    "        batch.pop('input_ids')\n",
    "        attention_mask = batch.attention_mask\n",
    "        batch.pop('attention_mask')\n",
    "        graph_batch = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_graph, x_text = model(graph_batch.to(device), \n",
    "                                    input_ids=input_ids.to(device), \n",
    "                                    attention_mask=attention_mask.to(device))\n",
    "            current_loss = criterion(x_graph, x_text)   \n",
    "            val_loss += current_loss.item()\n",
    "            \n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"save_path = os.path.join('./checkpoints', 'ep'+str(8)+model_save_name+'.pt')\\n\\ncheckpoint = torch.load(save_path)\\nmodel.load_state_dict(checkpoint['model_state_dict'])\\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\\nlr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "\"\"\"save_path = os.path.join('./checkpoints', 'ep'+str(8)+model_save_name+'.pt')\n",
    "\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----EPOCH 1-----\n",
      "Iteration: 50, Time: 50.1327 s, training loss: 2.0906\n",
      "Iteration: 100, Time: 99.1638 s, training loss: 1.8220\n",
      "Iteration: 150, Time: 148.1938 s, training loss: 1.4758\n",
      "Iteration: 200, Time: 197.4885 s, training loss: 1.2807\n",
      "Iteration: 250, Time: 246.2370 s, training loss: 1.1419\n",
      "Iteration: 300, Time: 289.0047 s, training loss: 0.9977\n",
      "Iteration: 350, Time: 330.6434 s, training loss: 0.8866\n",
      "Iteration: 400, Time: 373.4468 s, training loss: 0.8167\n",
      "Iteration: 450, Time: 415.7072 s, training loss: 0.8110\n",
      "Iteration: 500, Time: 458.3068 s, training loss: 0.6704\n",
      "Iteration: 550, Time: 500.8691 s, training loss: 0.7080\n",
      "Iteration: 600, Time: 543.1417 s, training loss: 0.6862\n",
      "Iteration: 650, Time: 586.4333 s, training loss: 0.5860\n",
      "Iteration: 700, Time: 629.7994 s, training loss: 0.6038\n",
      "Iteration: 750, Time: 672.4020 s, training loss: 0.5333\n",
      "Iteration: 800, Time: 717.4621 s, training loss: 0.5464\n",
      "Iteration: 850, Time: 765.1645 s, training loss: 0.5289\n",
      "Iteration: 900, Time: 809.1361 s, training loss: 0.4999\n",
      "Iteration: 950, Time: 854.3951 s, training loss: 0.4421\n",
      "Iteration: 1000, Time: 899.7966 s, training loss: 0.4504\n",
      "Iteration: 1050, Time: 945.3834 s, training loss: 0.4280\n",
      "Iteration: 1100, Time: 991.1830 s, training loss: 0.4452\n",
      "-----EPOCH 1----- done.  Validation loss:  0.4975618827246923\n",
      "checkpoint saved to: ./checkpoints/ep0text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 2-----\n",
      "Iteration: 1150, Time: 1098.7258 s, training loss: 0.3836\n",
      "Iteration: 1200, Time: 1148.4000 s, training loss: 0.3988\n",
      "Iteration: 1250, Time: 1197.8339 s, training loss: 0.3514\n",
      "Iteration: 1300, Time: 1245.7661 s, training loss: 0.3647\n",
      "Iteration: 1350, Time: 1293.9127 s, training loss: 0.3243\n",
      "Iteration: 1400, Time: 1342.1639 s, training loss: 0.3067\n",
      "Iteration: 1450, Time: 1386.6525 s, training loss: 0.3184\n",
      "Iteration: 1500, Time: 1430.7070 s, training loss: 0.3133\n",
      "Iteration: 1550, Time: 1476.0842 s, training loss: 0.2931\n",
      "Iteration: 1600, Time: 1519.9773 s, training loss: 0.3298\n",
      "Iteration: 1650, Time: 1564.2397 s, training loss: 0.2899\n",
      "Iteration: 1700, Time: 1608.1799 s, training loss: 0.3149\n",
      "Iteration: 1750, Time: 1652.3043 s, training loss: 0.3261\n",
      "Iteration: 1800, Time: 1699.9063 s, training loss: 0.3137\n",
      "Iteration: 1850, Time: 1740.8180 s, training loss: 0.2965\n",
      "Iteration: 1900, Time: 1781.2632 s, training loss: 0.2757\n",
      "Iteration: 1950, Time: 1822.1791 s, training loss: 0.2918\n",
      "Iteration: 2000, Time: 1861.6324 s, training loss: 0.2622\n",
      "Iteration: 2050, Time: 1903.3011 s, training loss: 0.2926\n",
      "Iteration: 2100, Time: 1942.3271 s, training loss: 0.2881\n",
      "Iteration: 2150, Time: 1983.1332 s, training loss: 0.2536\n",
      "Iteration: 2200, Time: 2022.6050 s, training loss: 0.2578\n",
      "-----EPOCH 2----- done.  Validation loss:  0.3275220056397606\n",
      "checkpoint saved to: ./checkpoints/ep1text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 3-----\n",
      "Iteration: 2250, Time: 2112.6658 s, training loss: 0.2004\n",
      "Iteration: 2300, Time: 2154.4050 s, training loss: 0.1877\n",
      "Iteration: 2350, Time: 2194.1129 s, training loss: 0.2015\n",
      "Iteration: 2400, Time: 2234.5364 s, training loss: 0.1893\n",
      "Iteration: 2450, Time: 2276.6745 s, training loss: 0.1862\n",
      "Iteration: 2500, Time: 2320.3992 s, training loss: 0.1905\n",
      "Iteration: 2550, Time: 2364.6177 s, training loss: 0.1923\n",
      "Iteration: 2600, Time: 2409.1122 s, training loss: 0.2184\n",
      "Iteration: 2650, Time: 2452.9349 s, training loss: 0.1892\n",
      "Iteration: 2700, Time: 2496.4672 s, training loss: 0.2459\n",
      "Iteration: 2750, Time: 2541.3051 s, training loss: 0.1847\n",
      "Iteration: 2800, Time: 2584.9493 s, training loss: 0.1921\n",
      "Iteration: 2850, Time: 2628.7422 s, training loss: 0.1889\n",
      "Iteration: 2900, Time: 2673.0172 s, training loss: 0.2150\n",
      "Iteration: 2950, Time: 2717.4657 s, training loss: 0.2368\n",
      "Iteration: 3000, Time: 2761.2514 s, training loss: 0.1904\n",
      "Iteration: 3050, Time: 2804.7426 s, training loss: 0.1930\n",
      "Iteration: 3100, Time: 2849.4326 s, training loss: 0.2188\n",
      "Iteration: 3150, Time: 2894.0955 s, training loss: 0.1965\n",
      "Iteration: 3200, Time: 2937.4152 s, training loss: 0.1717\n",
      "Iteration: 3250, Time: 2980.6803 s, training loss: 0.1862\n",
      "Iteration: 3300, Time: 3024.0833 s, training loss: 0.2001\n",
      "-----EPOCH 3----- done.  Validation loss:  0.26938061659310586\n",
      "checkpoint saved to: ./checkpoints/ep2text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 4-----\n",
      "Iteration: 3350, Time: 3120.7966 s, training loss: 0.1452\n",
      "Iteration: 3400, Time: 3164.5620 s, training loss: 0.1299\n",
      "Iteration: 3450, Time: 3206.3140 s, training loss: 0.1522\n",
      "Iteration: 3500, Time: 3249.2771 s, training loss: 0.1523\n",
      "Iteration: 3550, Time: 3291.1168 s, training loss: 0.1459\n",
      "Iteration: 3600, Time: 3335.9591 s, training loss: 0.1322\n",
      "Iteration: 3650, Time: 3378.0121 s, training loss: 0.1316\n",
      "Iteration: 3700, Time: 3418.4464 s, training loss: 0.1432\n",
      "Iteration: 3750, Time: 3458.7189 s, training loss: 0.1590\n",
      "Iteration: 3800, Time: 3500.8164 s, training loss: 0.1527\n",
      "Iteration: 3850, Time: 3540.9476 s, training loss: 0.1478\n",
      "Iteration: 3900, Time: 3581.3074 s, training loss: 0.1365\n",
      "Iteration: 3950, Time: 3623.2431 s, training loss: 0.1555\n",
      "Iteration: 4000, Time: 3667.1718 s, training loss: 0.1546\n",
      "Iteration: 4050, Time: 3712.6800 s, training loss: 0.1579\n",
      "Iteration: 4100, Time: 3757.9652 s, training loss: 0.1383\n",
      "Iteration: 4150, Time: 3803.0285 s, training loss: 0.1417\n",
      "Iteration: 4200, Time: 3847.6253 s, training loss: 0.1498\n",
      "Iteration: 4250, Time: 3894.7314 s, training loss: 0.1299\n",
      "Iteration: 4300, Time: 3938.3029 s, training loss: 0.1364\n",
      "Iteration: 4350, Time: 3980.2525 s, training loss: 0.1230\n",
      "Iteration: 4400, Time: 4021.5619 s, training loss: 0.1279\n",
      "-----EPOCH 4----- done.  Validation loss:  0.21323121734340306\n",
      "checkpoint saved to: ./checkpoints/ep3text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 5-----\n",
      "Iteration: 4450, Time: 4115.4677 s, training loss: 0.1122\n",
      "Iteration: 4500, Time: 4155.4468 s, training loss: 0.1268\n",
      "Iteration: 4550, Time: 4196.6116 s, training loss: 0.1186\n",
      "Iteration: 4600, Time: 4237.4544 s, training loss: 0.1278\n",
      "Iteration: 4650, Time: 4280.2074 s, training loss: 0.1214\n",
      "Iteration: 4700, Time: 4323.5691 s, training loss: 0.1037\n",
      "Iteration: 4750, Time: 4368.1497 s, training loss: 0.1153\n",
      "Iteration: 4800, Time: 4408.9575 s, training loss: 0.1103\n",
      "Iteration: 4850, Time: 4455.5602 s, training loss: 0.1324\n",
      "Iteration: 4900, Time: 4501.9895 s, training loss: 0.1239\n",
      "Iteration: 4950, Time: 4544.8380 s, training loss: 0.1183\n",
      "Iteration: 5000, Time: 4587.4875 s, training loss: 0.1423\n",
      "Iteration: 5050, Time: 4629.7351 s, training loss: 0.1215\n",
      "Iteration: 5100, Time: 4670.9295 s, training loss: 0.1430\n",
      "Iteration: 5150, Time: 4711.7686 s, training loss: 0.1278\n",
      "Iteration: 5200, Time: 4752.1674 s, training loss: 0.1248\n",
      "Iteration: 5250, Time: 4794.7734 s, training loss: 0.1379\n",
      "Iteration: 5300, Time: 4838.0671 s, training loss: 0.1205\n",
      "Iteration: 5350, Time: 4879.5950 s, training loss: 0.0989\n",
      "Iteration: 5400, Time: 4922.5407 s, training loss: 0.1175\n",
      "Iteration: 5450, Time: 4962.0599 s, training loss: 0.1149\n",
      "Iteration: 5500, Time: 5003.8754 s, training loss: 0.1234\n",
      "-----EPOCH 5----- done.  Validation loss:  0.20246370004764636\n",
      "checkpoint saved to: ./checkpoints/ep4text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 6-----\n",
      "Iteration: 5550, Time: 5098.8184 s, training loss: 0.1032\n",
      "Iteration: 5600, Time: 5139.6147 s, training loss: 0.1183\n",
      "Iteration: 5650, Time: 5178.9324 s, training loss: 0.0943\n",
      "Iteration: 5700, Time: 5219.1780 s, training loss: 0.0907\n",
      "Iteration: 5750, Time: 5259.6010 s, training loss: 0.0875\n",
      "Iteration: 5800, Time: 5300.1208 s, training loss: 0.0945\n",
      "Iteration: 5850, Time: 5341.1648 s, training loss: 0.0910\n",
      "Iteration: 5900, Time: 5382.1301 s, training loss: 0.0969\n",
      "Iteration: 5950, Time: 5423.8589 s, training loss: 0.0935\n",
      "Iteration: 6000, Time: 5467.3186 s, training loss: 0.1091\n",
      "Iteration: 6050, Time: 5510.1399 s, training loss: 0.0933\n",
      "Iteration: 6100, Time: 5552.3111 s, training loss: 0.0929\n",
      "Iteration: 6150, Time: 5594.0385 s, training loss: 0.0822\n",
      "Iteration: 6200, Time: 5635.8616 s, training loss: 0.0976\n",
      "Iteration: 6250, Time: 5678.2900 s, training loss: 0.0971\n",
      "Iteration: 6300, Time: 5721.1566 s, training loss: 0.1020\n",
      "Iteration: 6350, Time: 5764.2837 s, training loss: 0.0917\n",
      "Iteration: 6400, Time: 5807.1803 s, training loss: 0.1107\n",
      "Iteration: 6450, Time: 5849.3468 s, training loss: 0.0921\n",
      "Iteration: 6500, Time: 5891.2406 s, training loss: 0.1023\n",
      "Iteration: 6550, Time: 5934.5148 s, training loss: 0.1004\n",
      "Iteration: 6600, Time: 5977.0150 s, training loss: 0.0954\n",
      "-----EPOCH 6----- done.  Validation loss:  0.18410011990868919\n",
      "checkpoint saved to: ./checkpoints/ep5text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 7-----\n",
      "Iteration: 6650, Time: 6073.5783 s, training loss: 0.0674\n",
      "Iteration: 6700, Time: 6117.0622 s, training loss: 0.0756\n",
      "Iteration: 6750, Time: 6159.7265 s, training loss: 0.1067\n",
      "Iteration: 6800, Time: 6201.6945 s, training loss: 0.0883\n",
      "Iteration: 6850, Time: 6243.9982 s, training loss: 0.0809\n",
      "Iteration: 6900, Time: 6287.8883 s, training loss: 0.0839\n",
      "Iteration: 6950, Time: 6332.6308 s, training loss: 0.0879\n",
      "Iteration: 7000, Time: 6376.5336 s, training loss: 0.0786\n",
      "Iteration: 7050, Time: 6421.1970 s, training loss: 0.0834\n",
      "Iteration: 7100, Time: 6465.5673 s, training loss: 0.0777\n",
      "Iteration: 7150, Time: 6506.5209 s, training loss: 0.0731\n",
      "Iteration: 7200, Time: 6545.9708 s, training loss: 0.0750\n",
      "Iteration: 7250, Time: 6587.5992 s, training loss: 0.0801\n",
      "Iteration: 7300, Time: 6629.2184 s, training loss: 0.0813\n",
      "Iteration: 7350, Time: 6669.3295 s, training loss: 0.0837\n",
      "Iteration: 7400, Time: 6709.6430 s, training loss: 0.0855\n",
      "Iteration: 7450, Time: 6750.9998 s, training loss: 0.0961\n",
      "Iteration: 7500, Time: 6793.3167 s, training loss: 0.0888\n",
      "Iteration: 7550, Time: 6834.4422 s, training loss: 0.0775\n",
      "Iteration: 7600, Time: 6876.5323 s, training loss: 0.0918\n",
      "Iteration: 7650, Time: 6919.1153 s, training loss: 0.0901\n",
      "Iteration: 7700, Time: 6960.4195 s, training loss: 0.0738\n",
      "-----EPOCH 7----- done.  Validation loss:  0.18155128716369986\n",
      "checkpoint saved to: ./checkpoints/ep6text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 8-----\n",
      "Iteration: 7750, Time: 7050.0805 s, training loss: 0.0594\n",
      "Iteration: 7800, Time: 7090.4011 s, training loss: 0.0647\n",
      "Iteration: 7850, Time: 7130.5898 s, training loss: 0.0663\n",
      "Iteration: 7900, Time: 7170.0608 s, training loss: 0.0645\n",
      "Iteration: 7950, Time: 7210.5501 s, training loss: 0.0630\n",
      "Iteration: 8000, Time: 7250.9127 s, training loss: 0.0762\n",
      "Iteration: 8050, Time: 7290.4997 s, training loss: 0.0672\n",
      "Iteration: 8100, Time: 7330.8741 s, training loss: 0.0845\n",
      "Iteration: 8150, Time: 7370.3212 s, training loss: 0.0708\n",
      "Iteration: 8200, Time: 7410.4052 s, training loss: 0.0662\n",
      "Iteration: 8250, Time: 7450.6222 s, training loss: 0.0774\n",
      "Iteration: 8300, Time: 7490.0014 s, training loss: 0.0776\n",
      "Iteration: 8350, Time: 7528.7335 s, training loss: 0.0754\n",
      "Iteration: 8400, Time: 7568.6521 s, training loss: 0.0766\n",
      "Iteration: 8450, Time: 7607.7889 s, training loss: 0.0657\n",
      "Iteration: 8500, Time: 7647.1995 s, training loss: 0.0723\n",
      "Iteration: 8550, Time: 7687.0845 s, training loss: 0.0656\n",
      "Iteration: 8600, Time: 7728.0203 s, training loss: 0.0589\n",
      "Iteration: 8650, Time: 7769.1814 s, training loss: 0.0547\n",
      "Iteration: 8700, Time: 7809.5715 s, training loss: 0.0655\n",
      "Iteration: 8750, Time: 7851.4250 s, training loss: 0.0777\n",
      "Iteration: 8800, Time: 7892.0149 s, training loss: 0.0729\n",
      "-----EPOCH 8----- done.  Validation loss:  0.17643036585647806\n",
      "checkpoint saved to: ./checkpoints/ep7text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 9-----\n",
      "Iteration: 8850, Time: 7982.0319 s, training loss: 0.0686\n",
      "Iteration: 8900, Time: 8024.0907 s, training loss: 0.0688\n",
      "Iteration: 8950, Time: 8069.1973 s, training loss: 0.0639\n",
      "Iteration: 9000, Time: 8112.9448 s, training loss: 0.0615\n",
      "Iteration: 9050, Time: 8155.1191 s, training loss: 0.0619\n",
      "Iteration: 9100, Time: 8196.3683 s, training loss: 0.0795\n",
      "Iteration: 9150, Time: 8238.3072 s, training loss: 0.0492\n",
      "Iteration: 9200, Time: 8280.8968 s, training loss: 0.0643\n",
      "Iteration: 9250, Time: 8322.4395 s, training loss: 0.0621\n",
      "Iteration: 9300, Time: 8373.4752 s, training loss: 0.0715\n",
      "Iteration: 9350, Time: 8422.6251 s, training loss: 0.0617\n",
      "Iteration: 9400, Time: 8469.9210 s, training loss: 0.0599\n",
      "Iteration: 9450, Time: 8513.5002 s, training loss: 0.0604\n",
      "Iteration: 9500, Time: 8555.3849 s, training loss: 0.0514\n",
      "Iteration: 9550, Time: 8598.2770 s, training loss: 0.0518\n",
      "Iteration: 9600, Time: 8643.8962 s, training loss: 0.0575\n",
      "Iteration: 9650, Time: 8692.1168 s, training loss: 0.0515\n",
      "Iteration: 9700, Time: 8740.2391 s, training loss: 0.0558\n",
      "Iteration: 9750, Time: 8785.7483 s, training loss: 0.0512\n",
      "Iteration: 9800, Time: 8831.2635 s, training loss: 0.0571\n",
      "Iteration: 9850, Time: 8873.0852 s, training loss: 0.0619\n",
      "Iteration: 9900, Time: 8916.8604 s, training loss: 0.0502\n",
      "-----EPOCH 9----- done.  Validation loss:  0.17067420739389733\n",
      "checkpoint saved to: ./checkpoints/ep8text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n",
      "-----EPOCH 10-----\n",
      "Iteration: 9950, Time: 9013.8798 s, training loss: 0.0463\n",
      "Iteration: 10000, Time: 9056.3231 s, training loss: 0.0546\n",
      "Iteration: 10050, Time: 9098.7193 s, training loss: 0.0530\n",
      "Iteration: 10100, Time: 9142.1324 s, training loss: 0.0547\n",
      "Iteration: 10150, Time: 9183.7574 s, training loss: 0.0608\n",
      "Iteration: 10200, Time: 9225.2000 s, training loss: 0.0556\n",
      "Iteration: 10250, Time: 9266.6365 s, training loss: 0.0490\n",
      "Iteration: 10300, Time: 9309.0560 s, training loss: 0.0497\n",
      "Iteration: 10350, Time: 9352.4462 s, training loss: 0.0518\n",
      "Iteration: 10400, Time: 9395.5313 s, training loss: 0.0540\n",
      "Iteration: 10450, Time: 9439.5957 s, training loss: 0.0477\n",
      "Iteration: 10500, Time: 9483.2074 s, training loss: 0.0388\n",
      "Iteration: 10550, Time: 9526.6124 s, training loss: 0.0439\n",
      "Iteration: 10600, Time: 9569.7674 s, training loss: 0.0466\n",
      "Iteration: 10650, Time: 9614.6216 s, training loss: 0.0484\n",
      "Iteration: 10700, Time: 9658.1647 s, training loss: 0.0480\n",
      "Iteration: 10750, Time: 9706.9168 s, training loss: 0.0623\n",
      "Iteration: 10800, Time: 9753.3513 s, training loss: 0.0433\n",
      "Iteration: 10850, Time: 9802.0734 s, training loss: 0.0384\n",
      "Iteration: 10900, Time: 9848.6892 s, training loss: 0.0599\n",
      "Iteration: 10950, Time: 9894.2541 s, training loss: 0.0495\n",
      "Iteration: 11000, Time: 9941.4207 s, training loss: 0.0540\n",
      "-----EPOCH 10----- done.  Validation loss:  0.17478107125969508\n",
      "checkpoint saved to: ./checkpoints/ep9text_sentence-transformers-all-distilroberta-v1__gps_10_64_764m___base2_.pt\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "\n",
    "losses = []\n",
    "count_iter = 0\n",
    "time1 = time.time()\n",
    "printEvery = 50\n",
    "best_validation_loss = 1000000\n",
    "\n",
    "\n",
    "for i in range(epoch, nb_epochs):\n",
    "    print('-----EPOCH {}-----'.format(i+1))\n",
    "    losses, count_iter = train_one_epoch(model, train_loader, contrastive_loss, optimizer, losses, device, count_iter, printEvery, time1)\n",
    "\n",
    "    val_loss = eval(model, val_loader, contrastive_loss, device)\n",
    "    \n",
    "    best_validation_loss = min(best_validation_loss, val_loss)\n",
    "\n",
    "    print('-----EPOCH '+str(i+1)+'----- done.  Validation loss: ', str(val_loss/len(val_loader)) )\n",
    "    save_path = os.path.join('./checkpoints', 'ep' + str(i) + model_save_name+'.pt')\n",
    "    torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "        'validation_accuracy': val_loss,\n",
    "        'loss': losses[-1],\n",
    "        }, save_path)\n",
    "    print('checkpoint saved to: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
